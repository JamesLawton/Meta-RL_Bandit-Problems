{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-14T09:46:07.473755Z",
     "start_time": "2017-06-14T09:46:07.436782Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.signal\n",
    "import multiprocessing\n",
    "import threading\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw\n",
    "from PIL import ImageFont\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "from random import choice\n",
    "from time import sleep\n",
    "from time import time\n",
    "\n",
    "def update_target_graph(from_scope,to_scope):\n",
    "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)\n",
    "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)\n",
    "\n",
    "    op_holder = []\n",
    "    for from_var,to_var in zip(from_vars,to_vars):\n",
    "        op_holder.append(to_var.assign(from_var))\n",
    "    return op_holder\n",
    "\n",
    "def normalized_columns_initializer(std=1.0):\n",
    "    def _initializer(shape, dtype=None, partition_info=None):\n",
    "        out = np.random.randn(*shape).astype(np.float32)\n",
    "        out *= std / np.sqrt(np.square(out).sum(axis=0, keepdims=True))\n",
    "        return tf.constant(out)\n",
    "    return _initializer\n",
    "\n",
    "def discount(x, gamma):\n",
    "    return scipy.signal.lfilter([1], [1, -gamma], x[::-1], axis=0)[::-1]    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bandit Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-14T09:46:07.966156Z",
     "start_time": "2017-06-14T09:46:07.930504Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class bandit_environments():\n",
    "    \n",
    "    def __init__(self,difficulty):\n",
    "        self.num_actions = 2\n",
    "        self.difficulty = difficulty\n",
    "        self.reset()\n",
    "    \n",
    "    \n",
    "        \n",
    "    def reset(self):\n",
    "        #Even though a timestep is not part of MetaRL, I set it as in the study in order to keep track of actions taken\n",
    "        self.timestep = 0\n",
    "        if self.difficulty == 'easy': bandit_prob = np.random.choice([0.9,0.1])\n",
    "        if self.difficulty == 'medium': bandit_prob = np.random.choice([0.75,0.25])\n",
    "        if self.difficulty == 'hard': bandit_prob = np.random.choice([0.6,0.4])\n",
    "        if self.difficulty == 'uniform': bandit_prob = np.random.uniform()\n",
    "        if self.difficulty != 'independent':\n",
    "            self.bandit = np.array([bandit_prob,1 - bandit_prob])\n",
    "        else:\n",
    "            self.bandit = np.random.uniform(size=2)\n",
    "\n",
    "        #Action for each of the bandits\n",
    "    def pullArm(self,action):\n",
    "        #Get a random number.\n",
    "        self.timestep += 1\n",
    "        bandit = self.bandit[action]\n",
    "        result = np.random.uniform()\n",
    "        #if the action taken by the bandit is greater than a sample from uniform distribution - give a reward\n",
    "        if result < bandit:\n",
    "            reward = 1\n",
    "        else:\n",
    "            #return a negative reward.\n",
    "            reward = 0\n",
    "        if self.timestep > 99: \n",
    "            done = True\n",
    "        else: done = False\n",
    "        return reward,done,self.timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-14T09:46:08.310232Z",
     "start_time": "2017-06-14T09:46:08.169416Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AC_Network():\n",
    "    def __init__(self,a_size,scope,trainer):\n",
    "        with tf.variable_scope(scope):\n",
    "            #previous rewards, actions and the current timestep as input\n",
    "            self.prev_rewards = tf.placeholder(shape=[None,1],dtype=tf.float32)\n",
    "            self.prev_actions = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "            self.timestep = tf.placeholder(shape =[None, 1], dtype=tf.float32)\n",
    "            #utilize one-hot encoding on previous actions\n",
    "            self.prev_actions_onehot = tf.one_hot(self.prev_actions,a_size,dtype=tf.float32)\n",
    "            \n",
    "            hidden = tf.concat([self.prev_rewards, self.prev_actions_onehot,self.timestep],1)\n",
    "            \n",
    "            \n",
    "            #Recurrent network for temporal dependencies\n",
    "            lstm_cell = tf.contrib.rnn.BasicLSTMCell(48,state_is_tuple=True)\n",
    "            c_init = np.zeros((1, lstm_cell.state_size.c), np.float32)\n",
    "            h_init = np.zeros((1, lstm_cell.state_size.h), np.float32)\n",
    "            self.state_init = [c_init, h_init]\n",
    "            c_in = tf.placeholder(tf.float32, [1, lstm_cell.state_size.c])\n",
    "            h_in = tf.placeholder(tf.float32, [1, lstm_cell.state_size.h])\n",
    "            self.state_in = (c_in, h_in)\n",
    "            rnn_in = tf.expand_dims(hidden, [0])\n",
    "            step_size = tf.shape(self.prev_rewards)[:1]\n",
    "            state_in = tf.contrib.rnn.LSTMStateTuple(c_in, h_in)\n",
    "            lstm_outputs, lstm_state = tf.nn.dynamic_rnn(\n",
    "                lstm_cell, rnn_in, initial_state=state_in, sequence_length=step_size,\n",
    "                time_major=False)\n",
    "            lstm_c, lstm_h = lstm_state\n",
    "            self.state_out = (lstm_c[:1, :], lstm_h[:1, :])\n",
    "            rnn_out = tf.reshape(lstm_outputs, [-1, 48])\n",
    "            self.actions = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "            self.actions_onehot=tf.one_hot(self.actions,a_size,dtype=tf.float32)\n",
    "                    \n",
    "            \n",
    "\n",
    "            #Output Layers\n",
    "            self.policy = slim.fully_connected(rnn_out,a_size,\n",
    "                activation_fn = tf.nn.softmax,\n",
    "                weights_initializer = normalized_columns_initializer(0.01),\n",
    "                biases_initializer=None)\n",
    "            self.value = slim.fully_connected(rnn_out,1,\n",
    "                activation_fn = None,\n",
    "                weights_initializer = normalized_columns_initializer(1.0),\n",
    "                biases_initializer=None)\n",
    "            \n",
    "            #loss functions and gradients only for local networks to then apply to the global network\n",
    "            \n",
    "            if scope != 'global':\n",
    "                self.target_v = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "                self.advantages = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "                self.responsible_outputs = tf.reduce_sum(self.policy * self.actions_onehot,[1])\n",
    "                \n",
    "                #loss function\n",
    "                self.value_loss = 0.5 *tf.reduce_sum(tf.square(self.target_v -\n",
    "                                                               tf.reshape(self.value,[-1])))\n",
    "                self.entropy = -tf.reduce_sum(self.policy *tf.log(self.policy + 1e-7))\n",
    "                self.policy_loss = -tf.reduce_sum(tf.log(self.responsible_outputs + 1e-7)*self.advantages)\n",
    "                self.loss = 0.5 * self.value_loss + self.policy_loss - self.entropy * 0.05\n",
    "                \n",
    "                #retrieve gradients from the local network (workers)\n",
    "                local_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n",
    "                self.gradients = tf.gradients(self.loss, local_vars)\n",
    "                self.var_norms = tf.global_norm(local_vars)\n",
    "                grads,self.grad_norms = tf.clip_by_global_norm(self.gradients,50.0)\n",
    "                \n",
    "                #apply gradients to global\n",
    "                global_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'global')\n",
    "                self.apply_grads = trainer.apply_gradients(zip(grads,global_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-14T09:46:09.007544Z",
     "start_time": "2017-06-14T09:46:08.594316Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create each worker to interact with environment asynchronously\n",
    "\n",
    "class Worker():\n",
    "    def __init__(self, game, name, a_size, trainer, model_path, global_episodes):\n",
    "        self.name = \"Agent_\" + str(name)\n",
    "        self.number = 'hard'\n",
    "        self.model_path = model_path\n",
    "        self.trainer = trainer\n",
    "        self.global_episodes = global_episodes\n",
    "        self.increment = self.global_episodes.assign_add(1)\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.episode_mean_values = []\n",
    "        self.summary_writer = tf.summary.FileWriter(\"train_\"+str(self.number))\n",
    "    \n",
    "        \n",
    "                \n",
    "        #copy the global parameters from the AC network and create a local copy\n",
    "        self.local_AC = AC_Network(a_size, self.name, trainer)\n",
    "        self.update_local_ops = update_target_graph('global', self.name)\n",
    "        self.env = game\n",
    "        \n",
    "    #Training paradigm\n",
    "    def train(self, rollout, sess, gamma, bootstrap_value):\n",
    "        #use rollout to simulate the system and update value functions\n",
    "        rollout = np.array(rollout)\n",
    "        actions = rollout[:,0]\n",
    "        rewards = rollout[:,1]\n",
    "        timesteps = rollout[:,2]\n",
    "        #creating a variable for previous rewards and actions for Meta-Learning\n",
    "        prev_rewards = [0] + rewards[:-1].tolist()\n",
    "        prev_actions = [0] + actions[:-1].tolist()\n",
    "        \n",
    "        values = rollout[:,4]\n",
    "        \n",
    "        self.pr = prev_rewards\n",
    "        self.pa = prev_actions\n",
    "        \n",
    "        \n",
    "        #Use the rewards and actions to generate the advantage and discounted returns\n",
    "        self.rewards_plus = np.asarray(rewards.tolist() + [bootstrap_value])\n",
    "        discounted_rewards = discount(self.rewards_plus, gamma)[:-1]\n",
    "        self.value_plus = np.asarray(values.tolist() + [bootstrap_value])\n",
    "        advantages = rewards + gamma * self.value_plus[1:] - self.value_plus[:-1]\n",
    "        advantages = discount(advantages, gamma)\n",
    "        \n",
    "        #Now, update global network using gradients\n",
    "        #generate the network statistics in a local dictionary to save\n",
    "        rnn_state = self.local_AC.state_init\n",
    "        feed_dict = {self.local_AC.target_v:discounted_rewards,\n",
    "                    self.local_AC.prev_rewards:np.vstack(prev_rewards),\n",
    "                    self.local_AC.prev_actions:prev_actions,\n",
    "                    self.local_AC.actions:actions,\n",
    "                    self.local_AC.timestep:np.vstack(timesteps),\n",
    "                    self.local_AC.advantages:advantages,\n",
    "                    self.local_AC.state_in[0]:rnn_state[0],\n",
    "                    self.local_AC.state_in[1]:rnn_state[1]}\n",
    "        v_l,p_l,e_l,g_n,v_n,_ = sess.run([self.local_AC.value_loss,\n",
    "                                         self.local_AC.policy_loss,\n",
    "                                         self.local_AC.entropy,\n",
    "                                         self.local_AC.grad_norms,\n",
    "                                         self.local_AC.var_norms,\n",
    "                                         self.local_AC.apply_grads],\n",
    "                                        feed_dict=feed_dict)\n",
    "        return v_l / len(rollout),p_l / len(rollout), e_l / len(rollout), g_n, v_n\n",
    "    \n",
    "    def work(self,gamma,sess,coord,saver,train):\n",
    "        episode_count = sess.run(self.global_episodes)\n",
    "        total_steps = 0\n",
    "        print (\"Starting worker \" + str(self.number))\n",
    "        with sess.as_default(), sess.graph.as_default():\n",
    "            while not coord.should_stop():\n",
    "                sess.run(self.update_local_ops)\n",
    "                episode_buffer = []\n",
    "                episode_values = []\n",
    "                episode_frames = []\n",
    "                episode_reward = [0,0]\n",
    "                episode_step_count = 0\n",
    "                d = False\n",
    "                r = 0\n",
    "                a = 0\n",
    "                t = 0\n",
    "                self.env.reset()\n",
    "                rnn_state = self.local_AC.state_init\n",
    "                \n",
    "                \n",
    "                while d == False:\n",
    "                    #take an action by using the probabilities generated from the policy network output\n",
    "                    a_dist, v, rnn_state_new = sess.run([self.local_AC.policy, self.local_AC.value,self.local_AC.state_out],\n",
    "                            feed_dict = {\n",
    "                            self.local_AC.prev_rewards:[[r]],\n",
    "                            self.local_AC.timestep:[[t]],\n",
    "                            self.local_AC.prev_actions:[a],\n",
    "                            self.local_AC.state_in[0]:rnn_state[0],\n",
    "                            self.local_AC.state_in[1]:rnn_state[1]})\n",
    "                    a = np.random.choice(a_dist[0],p=a_dist[0])\n",
    "                    a = np.argmax(a_dist == a)\n",
    "                \n",
    "                    rnn_state = rnn_state_new\n",
    "                    r, d, t = self.env.pullArm(a)\n",
    "                    episode_buffer.append([a, r, t, d, v[0,0]])\n",
    "                    episode_values.append(v[0,0])\n",
    "                    \n",
    "                    episode_reward[a] += r\n",
    "                    total_steps += 1\n",
    "                    episode_step_count += 1\n",
    "                \n",
    "                \n",
    "                self.episode_rewards.append(np.sum(episode_reward))\n",
    "                self.episode_lengths.append(episode_step_count)\n",
    "                self.episode_mean_values.append(np.mean(episode_values))\n",
    "            \n",
    "                #at the end of each episode, update the local network using the experience buffer\n",
    "                if len(episode_buffer) != 0 and train == True:\n",
    "                    v_l,p_l,e_l,g_n,v_n = self.train(episode_buffer,sess,gamma,0.0)\n",
    "\n",
    "\n",
    "                #Save statistics and model parameters periodically\n",
    "                if episode_count % 50 == 0 and episode_count != 0:\n",
    "                    if episode_count % 500 == 0 and self.name == 'Agent_0' and train == True:\n",
    "                        saver.save(sess,self.model_path+'/model-'+str(episode_count)+'.cptk')\n",
    "                        print ('Saved Model')\n",
    "            \n",
    "              \n",
    "                    \n",
    "                    \n",
    "                    mean_reward = np.mean(self.episode_rewards[-50:])\n",
    "                    mean_length = np.mean(self.episode_lengths[-50:])\n",
    "                    mean_value = np.mean(self.episode_mean_values[-50:])\n",
    "                    summary = tf.Summary()\n",
    "                    summary.value.add(tag='Perf/Reward', simple_value = float(mean_reward))\n",
    "                    summary.value.add(tag='Perf/Length', simple_value=float(mean_length))\n",
    "                    summary.value.add(tag='Perf/Value', simple_value=float(mean_value))\n",
    "\n",
    "                    if train == True:\n",
    "                        summary.value.add(tag='Losses/Value loss', simple_value=float(v_l))\n",
    "                        summary.value.add(tag='Losses/Policy Loss', simple_value=float(p_l))\n",
    "                        summary.value.add(tag='Losses/Entropy', simple_value=float(e_l))\n",
    "                        summary.value.add(tag='losses/Grad norm', simple_value=float(g_n))\n",
    "                        summary.value.add(tag='Losses/Var Norm', simple_value=float(v_n))\n",
    "\n",
    "                    self.summary_writer.add_summary(summary, episode_count)\n",
    "\n",
    "                    self.summary_writer.flush()\n",
    "                if self.name == 'Agent_0':\n",
    "                    sess.run(self.increment)\n",
    "                episode_count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-06-14T09:46:08.770Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gamma = .8 #Discount Rate for advantage estimation\n",
    "a_size = 2 \n",
    "load_model = True\n",
    "train = False\n",
    "model_path = './model_meta'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-06-14T09:46:09.000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model...\n",
      "INFO:tensorflow:Restoring parameters from ./model_meta/model-21000.cptk\n",
      "Starting worker hard\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "    \n",
    "if not os.path.exists('./frames'):\n",
    "    os.makedirs('./frames')\n",
    "\n",
    "    \n",
    "with tf.device(\"/cpu:0\"): \n",
    "    global_episodes = tf.Variable(0,dtype=tf.int32,name='global_episodes',trainable=False)\n",
    "    #Learning rate determined by the values in the study\n",
    "    trainer = tf.train.AdamOptimizer(learning_rate=1e-3)\n",
    "    master_network = AC_Network(a_size,'global',None) # Generate global network\n",
    "    #num_workers = multiprocessing.cpu_count() # Set workers to number of available CPU threads\n",
    "    num_workers = 1\n",
    "    workers = []\n",
    "    # Create worker classes\n",
    "    for i in range(num_workers):\n",
    "        workers.append(Worker(bandit_environments('hard'),i,a_size,trainer,model_path,global_episodes))\n",
    "    saver = tf.train.Saver(max_to_keep=5)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    coord = tf.train.Coordinator()\n",
    "    if load_model == True:\n",
    "        print ('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    worker_threads = []\n",
    "    for worker in workers:\n",
    "        worker_work = lambda: worker.work(gamma,sess,coord,saver,train)\n",
    "        thread = threading.Thread(target=(worker_work))\n",
    "        thread.start()\n",
    "        worker_threads.append(thread)\n",
    "    coord.join(worker_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
